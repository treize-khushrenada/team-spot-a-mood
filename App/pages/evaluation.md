#### Models Evaluation

Since our web application is based on the streamlit engine, we cannot store users' response data on how relevant the recommended songs are. So it is a challenge to verify the accuracy of the information retrieval architecture. Recommended by our project supervisor, we created 6 different personas with pre-conceived queries and expected results based on the query. With this, we can manually score the recommended songs using the pre-conceived personas. Based on the song's binary score of relevant or not-relevant, we then calculate the precision at N (Precision@N) or the number of accurate predictions at that rank. for each query prediction, we produced 10 recommended songs. With these 10 values of precision at N, we then calculated the average precision at N (AP@N). This average precision at N will represent each prediction precision for each query. We then will evaluate the fine-tuned model and based pre-trained model with the Mean Average Precision which is the average of all the average precision at N for each query of each model. The good thing about Average Precision at N is that it front loads the weight of the higher ranking, so if you have higher relevancy in the higher ranking of the recommended songs, you will get higher Average Precision at N despite the same amount of relevance. This is especially good for our evaluation since our model is distance based and should be ranked based on the distance of query embedding to song embeddings.

The result of our model evaluation is shown in the table below.