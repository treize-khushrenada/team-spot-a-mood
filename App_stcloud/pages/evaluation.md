#### Models Evaluation

Since our web application is based on the streamlit engine, we cannot store users response data of how relevant the recommended songs are. So it is a challenge to verify the accuracy of the information retrieval architecture. From recommendation from our project supervisor, we created 6 different personas with pre-concieve query and expected results based on the query. With this we can manually score the recommended songs using the pre-concieve personas. Based on the songs binary score of relevant or not-relevant, we then calculate the precision at N (Precision@N), or the number of accurate prediction at that rank. for each query prediction we produced 10 recommended songs. With this 10 values of precision at N, we then calulate the average precision at N (AP@N). This average precision at N will represent each prediction precision for each query. We then will evaluate the fine-tuned model and based pre-trained model using the Mean Average Precision which is the average of all the average precision at N for each query of each model. The good thing about Average Precision at N is that it front load the weight of the higher ranking, so if you have higher relevancy in the higher ranking of the recommended songs, you will get higher Average Precision at N despite the same amount of relevance. This is especially good for our models evaluation since our model is distance based and should be ranked based on those distance of qurey embedding to songs embeddings.

The result of our model evaluation is shown in the table below.